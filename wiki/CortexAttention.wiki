#summary Attention in Cortex
@@[Home] -> [SensorsResearch] -> [CortexAttention]
<wiki:toc max_depth="3"/>
----

= Attention in the Neocortex =

see 2007 - Model of covert attention and learning

== 2.1 Mammalian Neocortex ==

=== 2.1.1 Overview of the cortex ===

  * cerebral cortex is neocortex and old structures like hippocampus
  * most intelligent animals have larger absolute and relative neocortex
  * neocortex is responsible for things such as processing sensory information, learning associations, consciousness and working memory (compare to Jeff Hawkins - neocortex is responsible for prediction)
  * after evolution it receives input from all sources and thus can build world model
  * independently of the location on the cortex, it has the same six-layered columnar structure and the same micro-connectivity

=== 2.1.2 Learning Feature Representations ===

  * exceptional feature of the motor cortex is that it has power to control the muscles
  * higher levels represent longer lasting actions that are more abstract and consist of many primitive actions
  * when motor cortex attends to some actions, the muscles actually perform those actions
  * when an animal attends to some features in its sensations, the neural representations even in the lowest cortical levels adapt mostly to those features and not to others
  * feedback connections could also help in relating the neural representations to their context and thus give meaning to the raw bottom-up data
   * if the context does not change when the inputs change, the context could teach the neuron that the different inputs are actually instantiations of the same object
   * feedback connections could act as teaching signals for the neurons

=== 2.1.3 Attention in Neocortex ===

  * sensory cortex cannot represent all objects existing in the inputs at the same time - there is selective attention
  * neocortex can think about and perceive only few different things simultaneously (covert attention)
  * possible reasons of attention limitation:
   * *neuron has much synaptic information but little activity information* - active state cannot represent all the knowledge of the neuron simultaneously
   * *binding problem* - one simple feature can take part in representing many different things; when two population codes overlap, the features between the objects can get mixed up; the right combinations of features can be found if only few objects are attended at once; the simulated reality must be kept apart from the real world, and different future plans must be kept from interfering with each other
   * invariant reference frame for pattern recognition - the same objects can appear in many different forms (weak)
  * targets of attention are selected based on bottom-up saliency and high-level intentions
  * *saliency* can be:
   * high contrast
   * good continuity
   * unexpectedness - in spatial or temporal context; dangerous or new opportunity
   * attending to salient targets can be reasonable for alive being - improves probability of having real object
  * high-level *intentions* can be:
   * search for certain objects from the visual field
   * can be controlled to target any kind of features, like spatial locations or auditory frequency bands
   * top-down control signals are thought to originate in the *frontal cortex*
  * *biased-competition model of attention* - all attention emerges from local competition between different features in the neural hierarchies; this competition can be biased with top-down signal, allowing for search of certain features
   * competition between different spatial locations works similarly to that between different features
   * if a certain object is given attentional bias, then the location of that object is searched for
   * if the bias is given to some location, then the cortex will recognise the possible object in that location

=== 2.1.4 Architecture of Neocortex ===
  
  * *connections*
   * 1st-order: thalamic -> inputs (layer 4) -> contextual (layers 1-3) -> outputs (layers 5-6) -> higher-order thalamic
   * higher-order: thalamic -> inputs -> contextual -> outputs -> ...
  * *minicolumns* form larger *columns* and *hypercolumns*
  * all neurons within one minicolumn respond roughly to similar stimuli
  * *suggestions*:
   * some layers could represent the true state of the world 
   * other layers could represent unexpected novelties
   * some layers could form laterally coherent feature aggregates between neighbouring columns
  * crucial connection type is *feedback connections*
   * more numerous than the bottom-up connections
   * cannot cause activations in the target neurons, but only modulate the activations
   * originate from the near surroundings of that area
   * important set of connections is feedback from the next hierarchical level
  * great amount of contextual input into layer 1 comes from special context mediating *matrix cells* in the thalamus
  * bottom-up circuitry from thalamus to inputs layer (4), to contextual layer (2,3), to outputs layer (5,6) and back to thalamus consists of excitatory neurons
  * *inhibitory connections* can:
   * take part in regulating the activations in the cortex
   * implement competition between excitatory neurons
   * implement contextual “explaining away” of local neural representations
  * there are more global signals, carried by *chemical neuromodulators*, that affect large parts of the cortex similarly; they can modulate both the plasticity and activations in the neurons
   * control learning and memorising
   * control the balance between bottom-up and top-down inference
   * gate the contents of working memory

=== 2.1.5 Purpose of Neocortex ===

  * lots of learning happening in the subcortical structures as well
   * *basal ganglia* are considered to do *reinforcement learning*, which means learning those motor actions that are rewarding
   * *cerebellum* learns to contract the muscles on just the right time instants, resulting in *anticipative actions* and *smooth control*
  * cortex learns invariant representations for the complex information in the raw sensory data
  * additionally to representing the world state, it performs motor actions itself and plans the future
  * cortex incorporates the goals of the animal into its world model, and can thus make good action decisions:
   * motor cortex perceives what the animal does - cortex can associate the evolutionary fixed action patterns to different situations
   * global neuromodulation signals, such as dopamine, mediate information about rewards and punishments
   * working memory in the prefrontal cortex is thought to be gated by subcortical structures, which focuses attention on the animal goals

== 2.2 Models of learning regularities in the world ==

  * cortex is often modelled as using unsupervised learning
  * real cortex does receive teaching signals: the general contextual inputs from the thalamus matrix cells and the neuromodulators

=== 2.2.1 Generative models ===

  * generative models try to infer the original causes underlying the sensations
  * generative models assume a physical model F that produces the sensory inputs x:
{{{
x = F(s) + n
s - the physical causes, vector s can include information about the history of the world
n - sensory noise
}}}
  * generative models try to learn the world model F, and then use this in inferring the causes s from the inputs
  * *Bayesian network* is example of generative model, hierarchical statistical model
   * can be used to infer the abstract causes from partial sensory inputs
   * inference is computationally demanding
  * *Helmholtz machine*
   * inference/recognition can be done with one feed-forward activation sweep if distinct recognition model is added alongside the generative model
   * recognition model is consistent as learnt at the same time with generative model
  * error function can be the Kullback-Leibler divergence between the real sensory input probability distribution and that of the learnt generative model
  * for dimensionality of real world accurate generative model would be immensely large and slow to learn
  * generative models do not take into account the importance of different real world objects, or causes
  * cortex of the animal must decide what to represent
  * optimal way to make decisions is to maximise the expected utility of the consequences of the actions, leading to *Bayesian decision theory*
   * actions could be the decisions about what objects to represent
   * utilities are the utilities of representing different objects in the case they really exist in the current world state
   * utility of representing an object should be higher for those objects that are relevant for the behaviour of the animal
  * *cortical neural networks* must use information about the utility in two different time scales
   * synaptic weights of the network should be used to represent those aspects of the world that are important (concerns learning and corresponds to the *longer time scale*)
   * faster time scale corresponds to attention - select those targets that are the *most relevant*, for example dangerous or favourable

=== 2.2.2 Learning features ===

  * hierarchical pattern recognition network - higher level in the hierarchy learn increasingly invariant, abstract and global features and objects
  * *Neocognitron*
   * able to recognise visual objects despite their location in the picture
   * has a growing translational invariance when moving upward in the hierarchy - by alternating simple-cell and complex-cell layers
   * simple cells represent different features in their inputs
   * complex-cell layers make a dimension reduction by grouping simple cells with the same features in different locations together - complex feature is invariant with respect to the location
   * after dimension reduction, the next levels receive inputs from a larger areas - number of inputs into each area does not vary much; so additionally to higher invariance in higher layers, the units in higher layers also code for larger, more global shapes
   * if learn with *Hebb rule*, choose direction in the multidimensional input space which contains the most variance, or energy - but no reason why the most variant feature in the inputs would be generally the most useful one
  * *Independent component analysis* (ICA) tries to extract statistically independent features from the inputs
   * representing independent features and objects explicitly in different neurons is useful
   * when different neurons learn from the same inputs, they should do not all learn the same features
   * to drives the features apart: *competitive learning* (only the most active neurons learn - e.g. SOM) and *decorrelation*
   * to avoid decreasing information - *adding conscience* - frequent winners decrease their probability to win subsequent competitions
   * decorrelation means removing the correlations between the neural activations

=== 2.2.3 Guiding learning with context ===

  * Hebbian-like trace-learning rule:
{{{
dw = x * y`, 
y' is a leaky integrator of the neuron’s activation
even if y goes to zero, the activation stays on for a short time in y'
}}}
  * recognition of certain objects in one time instant guides classifying the next inputs to the same objects - slow feature analysis (SFA)
  * spatial context can mediate information both within one sense and from other senses
  * learning of behaviourally meaningful aspects of the world
   * contextual information from the motor cortex could help all the other parts of the cortex to learn motorically relevant representations
   * contextual signals from the subcortical structures could allow for relating primitive emotional and instinct information to the cortical representations
  * matrix cells in the thalamus are known to mediate *very general contextual* information to all parts of the cortex
  * lateral and top-down feedback connections in the cortex itself probably mediate much contextual information between different cortical areas
  * *denoising source separation* (DSS) - generic feature learning framework
   * sparse ICA, SFA and CCA components can be learnt with DSS
{{{
1. Sphere the inputs.
2. Extract activations of features from the inputs.
3. Use prior information about the features to denoise the activations.
4. Adapt the features with the denoised activations in Hebbian style. Keep the feature vectors as unit length.
}}}

== 2.3 Models Of Attention ==

=== 2.3.1 Dynamical neural networks ===

  * cortex is not a static feature extractor or a pattern recognition system, but dynamical system
  * state of the cortex affects perception
   * past inputs affect perception, because the cortex can do *temporal pattern recognition*
   * cortex has its own *intentions*
  * *Helmholtz machine through time* - temporal pattern recognition network; feed-forward fashion - has a “state”, but the higher layers do not have any effect on the lower ones
  * *hierarchical predictive-coding models*
   * class of models which use top-down connections alongside bottom-up ones to analyse both static and temporal inputs
   * each level tries to predict the activations in the lower levels, which then send the prediction error back to the higher levels
   * network converges to a state where the prediction error is minimised at each level
   * explain *extra-classical receptive fields*, such as those of *end-stopping cells* in V1
   * hierarchy in these models can integrate information temporally and spatially
   * neurons make compromise between the *lower level error*, the *current activation* and the *prediction from the higher layer* - leads to holistic, coherent interpretation of the inputs in a Gestalt style
   * neurons do not feed their actual activations, but only the prediction errors, to the higher layers - these neurons are experts in *focusing attention to novel features* in the world
   * they try to learn everything in the inputs equally well - do not have attention.

=== 2.3.2 Controlling the targets of attention ===
